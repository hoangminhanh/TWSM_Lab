{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Heity94/TWSM_Lab/blob/main/Homework/5_Class_Homework/PH_5_Homework_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries"
      ],
      "metadata": {
        "id": "L0HoGesYBC1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "TWSM_path = \"/content/drive/MyDrive/Colab_Notebooks/02_HWR\"\n",
        "\n",
        "#from TWSM import *\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import spacy\n",
        "## Import packages\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# run this from a normal command line\n",
        "#!python -m spacy download en_core_web_md #160MB\n",
        "\n",
        "#can I download this to a local file instead and load it fom drive?\n"
      ],
      "metadata": {
        "id": "4k1Km8WrvyWM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6389dc57-8d52-43e2-b40a-4a399f0c5bd7"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install whatlies"
      ],
      "metadata": {
        "id": "RaL0PTnTkiWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://stackoverflow.com/questions/56927602/unable-to-load-the-spacy-model-en-core-web-lg-on-google-colab\n",
        "\n",
        "Now, *** restart the colab runtime *** !!"
      ],
      "metadata": {
        "id": "n359prBLepXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## word2vec in spacy"
      ],
      "metadata": {
        "id": "sO_IQ0wFdJVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load the spacy model that you have installed\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "# process a sentence using the model\n",
        "doc = nlp(\"The sun is shining brightly today but the moon is not\")\n",
        "\n",
        "# It's that simple - all of the vectors and words are assigned after this point\n",
        "# Get the vector for 'text':\n",
        "doc[3].vector\n",
        "\n",
        "# Get the mean vector for the entire sentence (useful for sentence classification etc.)\n",
        "doc.vector"
      ],
      "metadata": {
        "id": "Wxk4NV5AdL04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Opposites are not necessarily different\n",
        "doc = nlp(\"I loved Narnia but hated Armageddon\")\n",
        "\n",
        "print(doc[1])\n",
        "print(doc[4])\n",
        "print(doc[1].similarity(doc[4]))\n",
        "print(doc[4].similarity(doc[1]))\n",
        "\n",
        "doc = nlp(\"The king and the queen are enjoying a sumptious breakfast today\")\n",
        "\n",
        "print(doc[1])\n",
        "print(doc[4])\n",
        "print(doc[1].similarity(doc[4]))\n",
        "print(doc[4].similarity(doc[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxqeG5FZfFna",
        "outputId": "25fcd049-48fe-45f6-93d6-40ec2c2ac1c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loved\n",
            "hated\n",
            "0.66889775\n",
            "0.66889775\n",
            "king\n",
            "queen\n",
            "0.72526103\n",
            "0.72526103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WhatLies module"
      ],
      "metadata": {
        "id": "HneOrIcxh9z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "king = nlp.vocab[\"king\"].vector\n",
        "man = nlp.vocab[\"man\"].vector\n",
        "queen = nlp.vocab[\"queen\"].vector\n",
        "woman = nlp.vocab[\"woman\"].vector\n",
        "\n",
        "#or:\n",
        "def w2v(w=\"king\"):\n",
        "  return nlp.vocab[w].vector\n",
        "\n",
        "king = w2v(\"king\")"
      ],
      "metadata": {
        "id": "QUURX-boh9A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from whatlies import EmbeddingSet\n",
        "from whatlies.language import SpacyLanguage\n",
        "\n",
        "lang = SpacyLanguage('en_core_web_md')\n",
        "words = ['cat', 'dog', 'fish', 'kitten', 'man', 'woman', 'king', 'queen', 'doctor', 'nurse']\n",
        "\n",
        "emb = lang[words]\n",
        "emb.plot_interactive(x_axis='man', y_axis='woman')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "KsUIQbjskc_R",
        "outputId": "173048ef-2a0d-4611-e2b5-93064c659ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-a34ef04abf1345bf986ee981c845bb24\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-a34ef04abf1345bf986ee981c845bb24\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-a34ef04abf1345bf986ee981c845bb24\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"circle\", \"size\": 60}, \"encoding\": {\"color\": {\"field\": \"\", \"legend\": null, \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"name\", \"type\": \"nominal\"}, {\"field\": \"original\", \"type\": \"nominal\"}], \"x\": {\"axis\": {\"title\": \"man\"}, \"field\": \"x_axis\", \"type\": \"quantitative\"}, \"y\": {\"axis\": {\"title\": \"woman\"}, \"field\": \"y_axis\", \"type\": \"quantitative\"}}, \"selection\": {\"selector001\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"title\": \"man vs. woman\"}, {\"mark\": {\"type\": \"text\", \"color\": \"black\", \"dx\": -15, \"dy\": 3}, \"encoding\": {\"text\": {\"field\": \"original\", \"type\": \"nominal\"}, \"x\": {\"field\": \"x_axis\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"y_axis\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-9772bd3fc4e5e10fbf898ed48ed7f201\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-9772bd3fc4e5e10fbf898ed48ed7f201\": [{\"x_axis\": 0.3758322596549988, \"y_axis\": 0.34616324305534363, \"name\": \"cat\", \"original\": \"cat\"}, {\"x_axis\": 0.4621913731098175, \"y_axis\": 0.4013059139251709, \"name\": \"dog\", \"original\": \"dog\"}, {\"x_axis\": 0.350157767534256, \"y_axis\": 0.2681156396865845, \"name\": \"fish\", \"original\": \"fish\"}, {\"x_axis\": 0.2800500690937042, \"y_axis\": 0.3301210403442383, \"name\": \"kitten\", \"original\": \"kitten\"}, {\"x_axis\": 1.0, \"y_axis\": 0.6816136837005615, \"name\": \"man\", \"original\": \"man\"}, {\"x_axis\": 0.8037664890289307, \"y_axis\": 1.0, \"name\": \"woman\", \"original\": \"woman\"}, {\"x_axis\": 0.45961007475852966, \"y_axis\": 0.27491992712020874, \"name\": \"king\", \"original\": \"king\"}, {\"x_axis\": 0.2914373576641083, \"y_axis\": 0.40253907442092896, \"name\": \"queen\", \"original\": \"queen\"}, {\"x_axis\": 0.4489893317222595, \"y_axis\": 0.4943573474884033, \"name\": \"doctor\", \"original\": \"doctor\"}, {\"x_axis\": 0.3271060585975647, \"y_axis\": 0.5211429595947266, \"name\": \"nurse\", \"original\": \"nurse\"}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.LayerChart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "he4x501G0cdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector Algebra**\n",
        "\n",
        "seems much more difficult than in gensim"
      ],
      "metadata": {
        "id": "JQhFmpmee0tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "queries = [w for w in nlp.vocab if w.is_lower and w.prob >= -15]\n",
        "\n",
        "def cos_sim(a, b):\n",
        "    return cosine_similarity(a.reshape(1, -1), b.reshape(1, -1))\n",
        "\n",
        "def most_similar_vec(vec, count=10):\n",
        "    by_similarity = sorted(queries, key=lambda w: cos_sim(w.vector, vec), reverse=True)\n",
        "    return [w.orth_ for w in by_similarity[:count]]\n",
        "\n",
        "vec = nlp('woman').vector + nlp('king').vector - nlp(\"man\").vector\n",
        "most_similar_vec(vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8Z_BTHmj4G2",
        "outputId": "0b6976f4-4038-4972-f297-e151a19fcda1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['king',\n",
              " 'queen',\n",
              " 'prince',\n",
              " 'princes',\n",
              " 'kings',\n",
              " 'princess',\n",
              " 'princesses',\n",
              " 'mermaid',\n",
              " 'royal',\n",
              " 'royals']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "queries"
      ],
      "metadata": {
        "id": "S6eNPWlZkDXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## word2vec in gensim"
      ],
      "metadata": {
        "id": "r8MsXo0HgE3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "#https://github.com/RaRe-Technologies/gensim-data\n",
        "wv = api.load('glove-wiki-gigaword-100')#128MB\n",
        "#wv = api.load('glove-wiki-gigaword-50')#65MB\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69CQJjxfgUvR",
        "outputId": "5a5273e2-9ccc-4a61-9319-79f0390d4cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gynWpK0w0eIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = [\n",
        "    ('car', 'minivan'),   # a minivan is a kind of car\n",
        "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
        "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
        "    ('car', 'cereal'),    # ... and so on\n",
        "    ('car', 'communism'),\n",
        "]\n",
        "for w1, w2 in pairs:\n",
        "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbLSL0D2g4Je",
        "outputId": "c52c88cd-f71b-43d8-e68a-e2f0c66bad71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'car'\t'minivan'\t0.67\n",
            "'car'\t'bicycle'\t0.69\n",
            "'car'\t'airplane'\t0.65\n",
            "'car'\t'cereal'\t0.12\n",
            "'car'\t'communism'\t0.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks\n",
        "\n",
        "1. Train a word2vec model of dimension $100$ on the IMD data. (considering words appearing in more than 50 documents) Save the model if you like.\n",
        "\n",
        "2. Compute the embedding for each review (average word2vec)\n",
        "\n",
        "3. Fit a keras classifier to the embedded reviews. (2 hidden layers of size 40 each) Report/Monitor the accuracy on the test data.\n",
        "\n",
        "4. Load the bing sentiment dictionary. Compute two separate embeddings for the negative and positive sentiments.\n",
        "\n",
        "5. Compute the similarity between these two vectors and a few selected reviews. Does it agree with their label?\n"
      ],
      "metadata": {
        "id": "eA-8MKLQjT1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "YhEZsOc7BVqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMD Movie Reviews**"
      ],
      "metadata": {
        "id": "e3ofzmU3fbUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
        "    num_words=10000)"
      ],
      "metadata": {
        "id": "inHlYXX-gCJx",
        "outputId": "6631f821-5879-4ab2-e4f9-e978cc55fcd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoding reviews back to text**"
      ],
      "metadata": {
        "id": "6xrmGZDsPfUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict(\n",
        "    [(value, key) for (key, value) in word_index.items()])\n",
        "decoded_review = \" \".join(\n",
        "    [reverse_word_index.get(i - 3, \"?\") for i in train_data[0]])\n",
        "\n",
        "decoded_review"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "muyre50UtroS",
        "outputId": "92f08891-37a2-4d31-a902-6bea007b0a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode train data\n",
        "N=len(train_data)\n",
        "decoded_reviews = [\"\" for x in range(N)]\n",
        "\n",
        "for j in range(N):\n",
        "  decoded_reviews[j] = \" \".join(\n",
        "    [reverse_word_index.get(i - 3, \"?\") for i in train_data[j]])\n",
        "  \n",
        "# Get rid of beginning \"?\"\n",
        "decoded_reviews = [review[2:] for review in decoded_reviews]"
      ],
      "metadata": {
        "id": "TnCCq-Dcg4U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec\n",
        "Train a word2vec model of dimension $100$ on the IMD data. (considering words appearing in more than 50 documents) Save the model if you like."
      ],
      "metadata": {
        "id": "R-Fn0Izm9MW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the dataset for the word2vec model\n",
        "corpus=[doc.split() for doc in decoded_reviews]\n",
        "\n",
        "# Train the model for embeddings of size 100 considering words appearing in more than 50 documents, default window=5\n",
        "model = Word2Vec(corpus, size=100, min_count=50)\n",
        "model.save(TWSM_path+'/00_data/word2vec_imdb.model')"
      ],
      "metadata": {
        "id": "D5MVG4r98lFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Average word2vec\n",
        "Compute the embedding for each review (average word2vec)"
      ],
      "metadata": {
        "id": "ocalYMLE_Ima"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8OM3aCWvEZC"
      },
      "source": [
        "In the following we will derive the corpus. Note that word2vec (as opposed to doc2vec) generates one embedding for each word in the document. These then need to be aggregated at a document level. The simplest way is to determine the average over all words, but you can also use other aggregators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFBqJ2VS2rYL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a59c7c-a3af-4299-c585-79b7c62bbd4b"
      },
      "source": [
        "# Document representation for the text\n",
        "corpus_w2v=[[model.wv[word] for word in doc if word in model.wv.vocab.keys()] for doc in corpus]\n",
        "positive=[i for i in range(len(corpus)) if len(corpus_w2v[i])>0]\n",
        "\n",
        "corpus_w2v2=[corpus_w2v[i] for i in positive]\n",
        "#data_lemma2=[data_lemma[i] for i in positive]\n",
        "\n",
        "# Document average representation\n",
        "corpus_w2v_avg_clean=[sum(words)/len(words) for words in corpus_w2v2]\n",
        "\n",
        "# This corpus can be used later in clustering and classification tasks\n",
        "print(corpus_w2v_avg_clean[10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.07425238  0.06747388  0.39230126 -0.36663225  0.21546191 -0.36204475\n",
            "  0.07142097  0.07992306  0.03126279 -0.51161313  0.12652317  0.4391952\n",
            " -0.16302423 -0.12542012  0.27796707  0.11315694 -0.1846122   0.07238373\n",
            "  0.45462906  0.01931621  0.1215534   0.03018505 -0.21474141 -0.00561327\n",
            "  0.24638928 -0.15758626 -0.16836031  0.21521203  0.06390181 -0.07232941\n",
            "  0.11160904  0.14934357 -0.30050156  0.19315217  0.30240217  0.12886333\n",
            "  0.15003869  0.05364362  0.10478669  0.14057606  0.18024667 -0.33982483\n",
            "  0.00996698  0.03019215 -0.03181847 -0.38736826 -0.02504646  0.09713174\n",
            "  0.2815634   0.46132156 -0.18515831 -0.38660303 -0.09607214 -0.20545292\n",
            "  0.44870573 -0.02355427  0.39900258 -0.41577318 -0.2690298  -0.14373663\n",
            "  0.03981441 -0.06252696 -0.4330292  -0.21920864 -0.24917193  0.06513843\n",
            "  0.6396006   0.5933016   0.10797528 -0.08458997 -0.09392422 -0.44205388\n",
            " -0.33044085  0.00815693  0.00944452 -0.29033545 -0.21981075 -0.16393258\n",
            " -0.42866853  0.1167855   0.02380345 -0.19003735  0.06491923 -0.37294996\n",
            " -0.45045215 -0.14345226 -0.25404736 -0.2555466  -0.07514463 -0.18100905\n",
            " -0.12672105  0.06775453 -0.00091381  0.07769222  0.01155721 -0.17561737\n",
            " -0.45145223 -0.23141432 -0.06667303  0.28540736]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keras Classifier\n",
        "Fit a keras classifier to the embedded reviews. (2 hidden layers of size 40 each) Report/Monitor the accuracy on the test data."
      ],
      "metadata": {
        "id": "Nu46w0LU8paj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the architecture\n",
        "clf = keras.Sequential()\n",
        "clf.add(layers.Dense(40, input_dim=100, activation='relu')) \n",
        "clf.add(layers.Dense(40, activation='relu'))\n",
        "clf.add(layers.Dense(1, activation='sigmoid')) "
      ],
      "metadata": {
        "id": "rpWlgsOjA8Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Complile\n",
        "clf.compile(optimizer='adam',\n",
        "    loss='binary_crossentropy', \n",
        "    metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "0BVx1BrWDIbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Early stopping\n",
        "es = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "kLYhZPOrRI8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit\n",
        "clf.fit(np.array(corpus_w2v_avg_clean), train_labels, epochs=100, validation_split=0.3, callbacks=[es])"
      ],
      "metadata": {
        "id": "w5QE8xD0DQIv",
        "outputId": "134b636a-9411-4172-c35f-409d5f074b5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "547/547 [==============================] - 2s 3ms/step - loss: 0.4607 - accuracy: 0.7850 - val_loss: 0.3850 - val_accuracy: 0.8307\n",
            "Epoch 2/100\n",
            "547/547 [==============================] - 1s 2ms/step - loss: 0.3891 - accuracy: 0.8274 - val_loss: 0.3707 - val_accuracy: 0.8392\n",
            "Epoch 3/100\n",
            "547/547 [==============================] - 2s 4ms/step - loss: 0.3794 - accuracy: 0.8327 - val_loss: 0.3707 - val_accuracy: 0.8344\n",
            "Epoch 4/100\n",
            "547/547 [==============================] - 2s 4ms/step - loss: 0.3767 - accuracy: 0.8332 - val_loss: 0.3887 - val_accuracy: 0.8236\n",
            "Epoch 5/100\n",
            "547/547 [==============================] - 2s 4ms/step - loss: 0.3744 - accuracy: 0.8348 - val_loss: 0.3827 - val_accuracy: 0.8312\n",
            "Epoch 6/100\n",
            "547/547 [==============================] - 1s 3ms/step - loss: 0.3679 - accuracy: 0.8370 - val_loss: 0.3897 - val_accuracy: 0.8279\n",
            "Epoch 7/100\n",
            "547/547 [==============================] - 2s 3ms/step - loss: 0.3641 - accuracy: 0.8387 - val_loss: 0.3644 - val_accuracy: 0.8419\n",
            "Epoch 8/100\n",
            "547/547 [==============================] - 2s 4ms/step - loss: 0.3616 - accuracy: 0.8389 - val_loss: 0.3634 - val_accuracy: 0.8388\n",
            "Epoch 9/100\n",
            "547/547 [==============================] - 2s 4ms/step - loss: 0.3594 - accuracy: 0.8447 - val_loss: 0.3622 - val_accuracy: 0.8401\n",
            "Epoch 10/100\n",
            "547/547 [==============================] - 1s 3ms/step - loss: 0.3569 - accuracy: 0.8429 - val_loss: 0.3631 - val_accuracy: 0.8408\n",
            "Epoch 11/100\n",
            "547/547 [==============================] - 1s 3ms/step - loss: 0.3542 - accuracy: 0.8433 - val_loss: 0.3689 - val_accuracy: 0.8395\n",
            "Epoch 12/100\n",
            "547/547 [==============================] - 2s 4ms/step - loss: 0.3499 - accuracy: 0.8459 - val_loss: 0.3647 - val_accuracy: 0.8391\n",
            "Epoch 13/100\n",
            "547/547 [==============================] - 2s 3ms/step - loss: 0.3496 - accuracy: 0.8463 - val_loss: 0.3623 - val_accuracy: 0.8392\n",
            "Epoch 14/100\n",
            "547/547 [==============================] - 2s 4ms/step - loss: 0.3455 - accuracy: 0.8478 - val_loss: 0.3815 - val_accuracy: 0.8295\n",
            "Epoch 15/100\n",
            "547/547 [==============================] - 1s 3ms/step - loss: 0.3425 - accuracy: 0.8501 - val_loss: 0.3724 - val_accuracy: 0.8389\n",
            "Epoch 16/100\n",
            "547/547 [==============================] - 1s 2ms/step - loss: 0.3420 - accuracy: 0.8492 - val_loss: 0.3629 - val_accuracy: 0.8384\n",
            "Epoch 17/100\n",
            "547/547 [==============================] - 2s 3ms/step - loss: 0.3390 - accuracy: 0.8499 - val_loss: 0.3743 - val_accuracy: 0.8377\n",
            "Epoch 18/100\n",
            "547/547 [==============================] - 1s 3ms/step - loss: 0.3352 - accuracy: 0.8525 - val_loss: 0.3558 - val_accuracy: 0.8461\n",
            "Epoch 19/100\n",
            "547/547 [==============================] - 1s 2ms/step - loss: 0.3336 - accuracy: 0.8555 - val_loss: 0.3611 - val_accuracy: 0.8429\n",
            "Epoch 20/100\n",
            "547/547 [==============================] - 1s 2ms/step - loss: 0.3328 - accuracy: 0.8550 - val_loss: 0.3816 - val_accuracy: 0.8297\n",
            "Epoch 21/100\n",
            "547/547 [==============================] - 1s 3ms/step - loss: 0.3292 - accuracy: 0.8570 - val_loss: 0.3604 - val_accuracy: 0.8467\n",
            "Epoch 22/100\n",
            "547/547 [==============================] - 1s 2ms/step - loss: 0.3276 - accuracy: 0.8593 - val_loss: 0.3641 - val_accuracy: 0.8461\n",
            "Epoch 23/100\n",
            "547/547 [==============================] - 1s 2ms/step - loss: 0.3237 - accuracy: 0.8603 - val_loss: 0.3758 - val_accuracy: 0.8349\n",
            "Epoch 24/100\n",
            "547/547 [==============================] - 1s 3ms/step - loss: 0.3218 - accuracy: 0.8593 - val_loss: 0.3604 - val_accuracy: 0.8455\n",
            "Epoch 25/100\n",
            "547/547 [==============================] - 1s 3ms/step - loss: 0.3189 - accuracy: 0.8616 - val_loss: 0.3652 - val_accuracy: 0.8397\n",
            "Epoch 26/100\n",
            "547/547 [==============================] - 1s 3ms/step - loss: 0.3192 - accuracy: 0.8615 - val_loss: 0.3697 - val_accuracy: 0.8396\n",
            "Epoch 27/100\n",
            "547/547 [==============================] - 1s 3ms/step - loss: 0.3146 - accuracy: 0.8637 - val_loss: 0.3686 - val_accuracy: 0.8389\n",
            "Epoch 28/100\n",
            "547/547 [==============================] - 1s 3ms/step - loss: 0.3156 - accuracy: 0.8650 - val_loss: 0.3695 - val_accuracy: 0.8393\n",
            "Epoch 29/100\n",
            "547/547 [==============================] - 1s 3ms/step - loss: 0.3135 - accuracy: 0.8660 - val_loss: 0.3712 - val_accuracy: 0.8393\n",
            "Epoch 30/100\n",
            "547/547 [==============================] - 1s 2ms/step - loss: 0.3100 - accuracy: 0.8663 - val_loss: 0.3935 - val_accuracy: 0.8281\n",
            "Epoch 31/100\n",
            "547/547 [==============================] - 1s 2ms/step - loss: 0.3086 - accuracy: 0.8661 - val_loss: 0.3716 - val_accuracy: 0.8440\n",
            "Epoch 32/100\n",
            "547/547 [==============================] - 1s 2ms/step - loss: 0.3048 - accuracy: 0.8673 - val_loss: 0.3728 - val_accuracy: 0.8385\n",
            "Epoch 33/100\n",
            "547/547 [==============================] - 1s 2ms/step - loss: 0.3051 - accuracy: 0.8702 - val_loss: 0.3787 - val_accuracy: 0.8415\n",
            "Epoch 34/100\n",
            "547/547 [==============================] - 1s 3ms/step - loss: 0.3016 - accuracy: 0.8694 - val_loss: 0.3783 - val_accuracy: 0.8408\n",
            "Epoch 35/100\n",
            "547/547 [==============================] - 1s 3ms/step - loss: 0.3027 - accuracy: 0.8681 - val_loss: 0.3864 - val_accuracy: 0.8363\n",
            "Epoch 36/100\n",
            "547/547 [==============================] - 1s 2ms/step - loss: 0.2997 - accuracy: 0.8713 - val_loss: 0.3850 - val_accuracy: 0.8425\n",
            "Epoch 37/100\n",
            "547/547 [==============================] - 1s 3ms/step - loss: 0.2974 - accuracy: 0.8737 - val_loss: 0.3861 - val_accuracy: 0.8340\n",
            "Epoch 38/100\n",
            "547/547 [==============================] - 1s 3ms/step - loss: 0.2947 - accuracy: 0.8734 - val_loss: 0.3864 - val_accuracy: 0.8412\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff733e90310>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode test data\n",
        "N=len(test_data)\n",
        "decoded_reviews_test = [\"\" for x in range(N)]\n",
        "\n",
        "for j in range(N):\n",
        "  decoded_reviews_test[j] = \" \".join(\n",
        "    [reverse_word_index.get(i - 3, \"?\") for i in test_data[j]])\n",
        "  \n",
        "# Get rid of beginning \"?\"\n",
        "decoded_reviews_test = [review[2:] for review in decoded_reviews_test]\n",
        "\n",
        "# Prepare the dataset for the word2vec model\n",
        "corpus_test=[doc.split() for doc in decoded_reviews_test]\n",
        "\n",
        "# Document representation for the text\n",
        "corpus_w2v_test=[[model.wv[word] for word in doc if word in model.wv.vocab.keys()] for doc in corpus_test]\n",
        "positive_test=[i for i in range(len(corpus_test)) if len(corpus_w2v_test[i])>0]\n",
        "\n",
        "corpus_w2v2_test=[corpus_w2v_test[i] for i in positive_test]\n",
        "\n",
        "# Document average representation\n",
        "corpus_w2v_avg_clean_test=[sum(words)/len(words) for words in corpus_w2v2_test]"
      ],
      "metadata": {
        "id": "dK1Fwwx8V3yW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check shape\n",
        "np.array(corpus_w2v_avg_clean_test).shape"
      ],
      "metadata": {
        "id": "wLDnUjBjWeph",
        "outputId": "94fe01e2-9ff2-40ef-b6b2-1ab1c9fc3e0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = clf.evaluate(np.array(corpus_w2v_avg_clean_test), test_labels, verbose=0)\n",
        "print(\"Accuracy on test_data:\", np.round(results[1],2))"
      ],
      "metadata": {
        "id": "2l9muBoOWz9J",
        "outputId": "a40e492d-ad0b-40ca-96c1-5733a8d584ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test_data: 0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bing sentiment embeddings\n",
        "Load the bing sentiment dictionary. Compute two separate embeddings for the negative and positive sentiments."
      ],
      "metadata": {
        "id": "zK0lTWmdAKDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load bing\n",
        "bing_df = pd.read_csv(TWSM_path+\"/00_data/SentimentDictionaries/bing.csv\", index_col=0)\n",
        "bing_df.sample(5)"
      ],
      "metadata": {
        "id": "ojfbavFUA8xg",
        "outputId": "af9a7d04-2ca8-41bc-d0c9-e21a21426ebd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            word sentiment\n",
              "6748      wowing  positive\n",
              "2612     gallant  positive\n",
              "130      affably  positive\n",
              "1524        dire  negative\n",
              "492   benevolent  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d8dc4064-37fc-4dfa-bb5c-fc743737f6a8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6748</th>\n",
              "      <td>wowing</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2612</th>\n",
              "      <td>gallant</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>affably</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1524</th>\n",
              "      <td>dire</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>492</th>\n",
              "      <td>benevolent</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8dc4064-37fc-4dfa-bb5c-fc743737f6a8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d8dc4064-37fc-4dfa-bb5c-fc743737f6a8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d8dc4064-37fc-4dfa-bb5c-fc743737f6a8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter bing df \n",
        "pos_df = bing_df[bing_df[\"sentiment\"]==\"positive\"]\n",
        "neg_df = bing_df[bing_df[\"sentiment\"]==\"negative\"]"
      ],
      "metadata": {
        "id": "JPeX6bGeZmiT"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if no word got lost during filtering ;)\n",
        "neg_df.shape[0]+pos_df.shape[0] == bing_df.shape[0]"
      ],
      "metadata": {
        "id": "PidOVqwgZm8T",
        "outputId": "5f0a0c87-92e9-4b74-cc1b-88bcbd14b74b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create corpus (reshape, because for Word2Vec it has to be in a list of lists format)\n",
        "corpus_pos = np.array(pos_df[\"word\"]).reshape(len(pos_df),1)\n",
        "corpus_neg = np.array(neg_df[\"word\"]).reshape(len(neg_df),1)"
      ],
      "metadata": {
        "id": "SQ0F79_koxs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model for embeddings of size 100 with min_count=1 (because there is only 1 \"sentence\")\n",
        "model_pos = Word2Vec(corpus_pos, size=100, min_count=1)\n",
        "model_neg = Word2Vec(corpus_neg, size=100, min_count=1)"
      ],
      "metadata": {
        "id": "FuYShXfSaQWh"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity between positive and negative embeddings\n",
        "Compute the similarity between these two vectors and a few selected reviews. Does it agree with their label?\n"
      ],
      "metadata": {
        "id": "P3_VlbebAh3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> â— **Please note:** The solution below is probably not correct, but I did not see another way on how to compute the overall similarity between a review and the embedded sentiment"
      ],
      "metadata": {
        "id": "_LIeYyb69knH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Average word2vec corpus for positive sentiment \n",
        "corpus_w2v_pos=[[model_pos.wv[word] for word in doc if word in model_pos.wv.vocab.keys()] for doc in corpus]\n",
        "positive=[i for i in range(len(corpus)) if len(corpus_w2v_pos[i])>0]\n",
        "\n",
        "corpus_w2v2_pos=[corpus_w2v_pos[i] for i in positive]\n",
        "decoded_reviews_pos=[decoded_reviews[i] for i in positive]\n",
        "\n",
        "# Document average representation\n",
        "corpus_w2v_avg_clean_pos=[sum(words)/len(words) for words in corpus_w2v2_pos]"
      ],
      "metadata": {
        "id": "TN5UUWIXbEaG"
      },
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average word2vec corpus for negative sentiment \n",
        "corpus_w2v_neg=[[model_neg.wv[word] for word in doc if word in model_neg.wv.vocab.keys()] for doc in corpus]\n",
        "positive=[i for i in range(len(corpus)) if len(corpus_w2v_neg[i])>0]\n",
        "\n",
        "corpus_w2v2_neg=[corpus_w2v_neg[i] for i in positive]\n",
        "decoded_reviews_neg=[decoded_reviews[i] for i in positive]\n",
        "\n",
        "# Document average representation\n",
        "corpus_w2v_avg_clean_neg=[sum(words)/len(words) for words in corpus_w2v2_neg]"
      ],
      "metadata": {
        "id": "KkM753UYuA3h"
      },
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similarity to 1st review (positive)"
      ],
      "metadata": {
        "id": "JkPezDTU3B4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if review is the same (some reviews were lost during averaging since no positive or negative words were included)\n",
        "decoded_reviews_pos[0] == decoded_reviews_neg[0]"
      ],
      "metadata": {
        "id": "zKFB0pr_4YNe",
        "outputId": "50564c55-048f-4fda-d1a6-d0f7e29a1335",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoded_reviews_pos[0])\n",
        "print(\"True label:\", train_labels[0])"
      ],
      "metadata": {
        "id": "XwB3G43V1wmZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c499a3f6-0aff-472c-ba08-09d482c5e2cd"
      },
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
            "True label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Positive connotated words in first review\n",
        "[word for word in corpus[0] if word in model_pos.wv.vocab.keys()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9n02KOp1pA3",
        "outputId": "912fb8b0-7bbe-4524-94ec-f7d97c3fcbef"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['brilliant',\n",
              " 'amazing',\n",
              " 'loved',\n",
              " 'witty',\n",
              " 'great',\n",
              " 'brilliant',\n",
              " 'recommend',\n",
              " 'amazing',\n",
              " 'good',\n",
              " 'brilliant',\n",
              " 'amazing',\n",
              " 'lovely']"
            ]
          },
          "metadata": {},
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Negative connotated words in first review\n",
        "[word for word in corpus[0] if word in model_neg.wv.vocab.keys()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4sWMya41PMT",
        "outputId": "ce591f79-cf5d-4453-dac9-3c1cb1baf83a"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sad', 'cry']"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Max cosine similarity to positive embedding:\", np.round(model_pos.wv.cosine_similarities(corpus_w2v_avg_clean_pos[0], model_pos.wv.vectors).max(),2))\n",
        "print(\"Avg. cosine similarity to positive embedding:\", np.round(model_pos.wv.cosine_similarities(corpus_w2v_avg_clean_pos[0], model_pos.wv.vectors).mean(),2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpeC7xaZ2F6q",
        "outputId": "d262bbcc-2189-4b8f-ae4f-927e41ed113d"
      },
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max cosine similarity to positive embedding: 0.66\n",
            "Avg. cosine similarity to positive embedding: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Max cosine similarity to negative embedding:\", np.round(model_neg.wv.cosine_similarities(corpus_w2v_avg_clean_neg[0], model_neg.wv.vectors).max(),2))\n",
        "print(\"Avg. cosine similarity to negative embedding:\", np.round(model_neg.wv.cosine_similarities(corpus_w2v_avg_clean_neg[0], model_neg.wv.vectors).mean(),2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztB9wtwG2fk6",
        "outputId": "e823759a-8b94-4d54-90b2-c12761068b0d"
      },
      "execution_count": 280,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max cosine similarity to negative embedding: 0.67\n",
            "Avg. cosine similarity to negative embedding: -0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similarity to 2nd review (negative)"
      ],
      "metadata": {
        "id": "vv_rPepF5mTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# index of review\n",
        "i = 1"
      ],
      "metadata": {
        "id": "XtpFAAbW7SuZ"
      },
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if review is the same (some reviews were lost during averaging since no positive or negative words were included)\n",
        "decoded_reviews_pos[i] == decoded_reviews_neg[i]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjskBF8u2vsq",
        "outputId": "abccb1ef-2428-4b9b-f80a-53ee6dba3ecd"
      },
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoded_reviews[i])\n",
        "print(\"True label:\", train_labels[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zOfM-Re2mjl",
        "outputId": "43cb8564-145d-4426-baae-01cbc4f4c982"
      },
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal ? the hair is big lots of boobs ? men wear those cut ? shirts that show off their ? sickening that men actually wore them and the music is just ? trash that plays over and over again in almost every scene there is trashy music boobs and ? taking away bodies and the gym still doesn't close for ? all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\n",
            "True label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Positive connotated words in first review\n",
        "[word for word in corpus[i] if word in model_pos.wv.vocab.keys()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aaf41f7-a25a-42ca-ee17-28009ce8b86b",
        "id": "u00TYNwk65ac"
      },
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['best', 'love', 'best', 'worked', 'charm', 'good']"
            ]
          },
          "metadata": {},
          "execution_count": 288
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Negative connotated words in first review\n",
        "[word for word in corpus[i] if word in model_neg.wv.vocab.keys()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fe6295c-547b-44f9-db04-f9cf9ea91352",
        "id": "wEOv1cAW65ad"
      },
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bad',\n",
              " 'terrible',\n",
              " 'cheesy',\n",
              " 'worst',\n",
              " 'plot',\n",
              " 'ridiculous',\n",
              " 'abomination',\n",
              " 'laughable',\n",
              " 'showdown',\n",
              " 'killer',\n",
              " 'damn',\n",
              " 'terribly',\n",
              " 'sickening',\n",
              " 'funny',\n",
              " 'sickening',\n",
              " 'trash',\n",
              " 'trashy',\n",
              " 'bad',\n",
              " 'disaster',\n",
              " 'bad']"
            ]
          },
          "metadata": {},
          "execution_count": 289
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Max cosine similarity to positive embedding:\", np.round(model_pos.wv.cosine_similarities(corpus_w2v_avg_clean_pos[i], model_pos.wv.vectors).max(),2))\n",
        "print(\"Avg. cosine similarity to positive embedding:\", np.round(model_pos.wv.cosine_similarities(corpus_w2v_avg_clean_pos[i], model_pos.wv.vectors).mean(),2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ef19dd-f7ae-40e3-c2b1-9dcccbaec7fd",
        "id": "MIpsZ6uq65ae"
      },
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max cosine similarity to positive embedding: 0.68\n",
            "Avg. cosine similarity to positive embedding: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Max cosine similarity to negative embedding:\", np.round(model_neg.wv.cosine_similarities(corpus_w2v_avg_clean_neg[i], model_neg.wv.vectors).max(),2))\n",
        "print(\"Avg. cosine similarity to negative embedding:\", np.round(model_neg.wv.cosine_similarities(corpus_w2v_avg_clean_neg[i], model_neg.wv.vectors).mean(),2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6367485-aedc-46fb-e2c6-7f7b7103eb8d",
        "id": "tg-_oQlf65af"
      },
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max cosine similarity to negative embedding: 0.54\n",
            "Avg. cosine similarity to negative embedding: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similarity to 3rd review (negative)"
      ],
      "metadata": {
        "id": "FwZbH8J27gWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# index of review\n",
        "i = 2"
      ],
      "metadata": {
        "id": "OkMK23Ao7gWM"
      },
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if review is the same (some reviews were lost during averaging since no positive or negative words were included)\n",
        "decoded_reviews_pos[i] == decoded_reviews_neg[i]"
      ],
      "metadata": {
        "outputId": "32478028-8f32-4f46-c479-76730432613f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u6fEnMp7gWN"
      },
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 293
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoded_reviews[i])\n",
        "print(\"True label:\", train_labels[i])"
      ],
      "metadata": {
        "outputId": "232959d5-71c7-4019-e624-02b10241b04a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZkH1t057gWN"
      },
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had ? working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how ? this is to watch save yourself an hour a bit of your life\n",
            "True label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Positive connotated words in first review\n",
        "[word for word in corpus[i] if word in model_pos.wv.vocab.keys()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9df12efd-cbdc-4e9f-9ca7-0b9ca5248426",
        "id": "BNrLLMkm7gWN"
      },
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['like', 'great', 'like']"
            ]
          },
          "metadata": {},
          "execution_count": 295
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Negative connotated words in first review\n",
        "[word for word in corpus[i] if word in model_neg.wv.vocab.keys()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4657c88e-d183-4b9b-9cf9-254b20b97da2",
        "id": "RFLLjHTa7gWO"
      },
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['worst', 'bad', 'feeble', 'excuse', 'crap', 'crap']"
            ]
          },
          "metadata": {},
          "execution_count": 296
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Most similar word to avg. embedding:\", model_pos.wv.similar_by_vector(corpus_w2v_avg_clean_pos[i], topn=1))\n",
        "print(\"Max cosine similarity to positive embedding:\", np.round(model_pos.wv.cosine_similarities(corpus_w2v_avg_clean_pos[i], model_pos.wv.vectors).max(),2))\n",
        "print(\"Avg. cosine similarity to positive embedding:\", np.round(model_pos.wv.cosine_similarities(corpus_w2v_avg_clean_pos[i], model_pos.wv.vectors).mean(),2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b987548b-fecb-49d8-e499-89d0ddeaef60",
        "id": "C_WWIXY_7gWO"
      },
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar word to avg. embedding: [('like', 0.913170337677002)]\n",
            "Max cosine similarity to positive embedding: 0.91\n",
            "Avg. cosine similarity to positive embedding: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Most similar word to avg. embedding:\", model_neg.wv.similar_by_vector(corpus_w2v_avg_clean_neg[i], topn=1))\n",
        "print(\"Max cosine similarity to negative embedding:\", np.round(model_neg.wv.cosine_similarities(corpus_w2v_avg_clean_neg[i], model_neg.wv.vectors).max(),2))\n",
        "print(\"Avg. cosine similarity to negative embedding:\", np.round(model_neg.wv.cosine_similarities(corpus_w2v_avg_clean_neg[i], model_neg.wv.vectors).mean(),2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92f63cc6-74ae-453e-9207-38528dd5f1af",
        "id": "qvu-EFCF7gWO"
      },
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar word to avg. embedding: [('crap', 0.7057374715805054)]\n",
            "Max cosine similarity to negative embedding: 0.71\n",
            "Avg. cosine similarity to negative embedding: 0.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Class5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}